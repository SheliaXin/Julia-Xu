<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    Notes For Deep Learning
    </title>
  <link href="https://fonts.googleapis.com/css?family=Ubuntu:400,400i,700" rel="stylesheet">
  <link rel="stylesheet" href="/Xin-Xu-Homepage/assets/css/all.css">
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


<body>
  <header class = "header ">

  <div class = "postheader">

  <nav>
    <a href="/Xin-Xu-Homepage/">Home</a>
  </nav>
</div>
</header>


   <article class="post-content">
     <h3 class="post-title">Notes For Deep Learning</h3>
     <p class="post-date">2017-09-04 21:05:54 -0700</p>
     <h4 id="neural-network-framework">1 Neural Network framework:</h4>
<ol>
  <li>Initialization of each parameters</li>
  <li>Repeat:
    <ul>
      <li>Forward propagation (Compute z, a, yPred, loss)</li>
      <li>Backward propagation (Compute gradients - dz, da; dw, db), and update parameters (gradient descent)</li>
    </ul>
  </li>
</ol>

<p><strong>Tips:</strong></p>

<ul>
  <li>The weights $W^{[l]}$ should be initialized randomly to break symmetry.</li>
  <li>If all $W^{[l]} &lt;1$ or $W^{[l]} &gt;1$ even slightly, the gradients will vanish/explode. We can reduce this problem by controlling the variance of $w$ during the initialization. For sigmoid, set $Var(w_i) = \frac{1}{n}$. For ReLu, set $Var(w_i) = \frac{2}{n^{[l-1]}}$ (He Initialization). For tanh, set  $Var(w_i) =\sqrt{\frac{1}{n^{[l-1]}}} $ or $\sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}$.</li>
  <li>Use <a href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">Gradient Checking</a> to debug. (See Details Course2 Week1)</li>
</ul>

<h4 id="activation-function-a-non-linear-function">Activation function: A Non-linear function</h4>
<ol>
  <li><strong>tanh</strong> function always works better than sigmoid function. tanh ( $tanh(z) = \frac{e^z - e^{-z}}{e^z +e^{-z}}$ ) function is a shifted version of sigmoid, but goes cross (0,0). The mean of its output is closer to zero, and so it centers the data better for the next layer.</li>
  <li>When z is very small/large, both sigmoid &amp; tanh has small slope. -&gt; slow down gradient descent. -&gt; <strong>Use RELU</strong> (rectified linear unit) (a = max(0,z)).</li>
  <li>One disadvantage of RELU is that the derivative is equal to zero when z is negative (In practice, it works just fine. another version is <strong>Leaky ReLu</strong> (a = max(0.01z, z)), often works better, but not used widely).</li>
</ol>

<p><strong>Tips:</strong></p>

<ul>
  <li>If you are doing binary classification, usually use sigmoid function as output layer.</li>
  <li>On other layers RELU is often the default choice of activation function.</li>
</ul>

<h4 id="gradient-descent">Gradient descent</h4>

<p><strong>Mini-batch Gradient Descent</strong>
- Gradient Descent use all data to update gradient, mini-batch gradient descent use part and stochastic gradient descent uses only 1.
- Tune a learning rate hyperparameter $\alpha$.
- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large). (Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.)</p>

<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using <strong>momentum</strong> can reduce these oscillations. Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps.</p>

<script type="math/tex; mode=display">\begin{cases}
v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\
W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}
\end{cases}\tag{3}</script>

<script type="math/tex; mode=display">\begin{cases}
v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\
b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}
\end{cases}\tag{4}</script>

<p>Two techs to speed up Gradient Descent:
- Gradient Descent with <strong>momentum</strong>
- <strong>RMSprop</strong> (root mean square prop)
(<strong>Adam optimization algorithm</strong> combine momentum and RMSprop together.)
- Learning rate decay.</p>

<h4 id="regularization-of-neural-network">Regularization of Neural Network</h4>

<p>Regularization is usually used to reduce overfitting.</p>

<ul>
  <li>L2-regularization (with Frobenius Norm, also known as “weight decay”). Its cost function will be:</li>
</ul>

<script type="math/tex; mode=display">J(w^{[1]}, b^{[1]}, \dots, w^{[L]}) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum_{l=1}^{L} \sum_{l=1}^{L} ||w^{[l]}||^2_{F}</script>

<blockquote>
  <p>where,</p>
</blockquote>

<script type="math/tex; mode=display">||w^{[l]}||^2_{F} = \sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2</script>

<blockquote>
  <p>By penalizing the square values of the weights in the cost function, it drives all the weights to smaller values. This leads to a smoother model in which the output changes more slowly as the input changes.</p>
</blockquote>

<ul>
  <li>
    <p>Dropout Regularization: Go through each layer of the Neural Network, randomly eliminate the nodes with a given probability. <tip>[!$a^[i]$ need to be updated by dividing the probability, to keep the expected value; Dropout only implemented on training set, not testing set.]</tip> The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p>
  </li>
  <li>
    <p>Other methods: Data augmentation, Early stopping.</p>
  </li>
</ul>

<h4 id="tuning-parameters">Tuning Parameters</h4>
<ul>
  <li>In deep learning, we usually recommend that you: <small>[From Coursera: Neural Networks and Deep Learning - HW1 ]</small>
    <ul>
      <li>Choose the learning rate that better minimizes the cost function.</li>
      <li>If your model overfits, use other techniques to reduce overfitting.</li>
    </ul>
  </li>
</ul>

   </article>


  <footer id="footer">

  <div class="lockup">

    <div class="content-wrap">
      <nav>
        
          <a href="#about-me">About Me</a>
        
          <a href="#posts">Blog</a>
        
          <a href="#projects">Project</a>
        
          <a href="#contact">Contact</a>
        
      </nav>

      <p class="copyright">All content copyright 2017</p>
    </div>

  </div>

</footer>

</body>

</html>
